import torch
import torch.optim as optim
import torch.utils.data
from Dataset import Dataset
from tqdm import tqdm 
from ppv_curve_utils import score, compute_PPV

def train_model(model, data_dict, struct_file, num_epoch, lr, dataloader_params, verbose=False, seed=10):
    """
    Implement a train loop. The model is sould be a transformer based on the one implemented in Transformer.py 
    parameters :
        model : Transformer, the model we are training
        data_dict : dict, the data dictionary generated by the read_fasta_utils.py
        struct_file: str, name of the file where the true structure of the protein family is stored
        num_epoch : int, the number of iteration for the process
        lr : float, the learning rate for the adam optimizer
        verbose : bool, should the validation loss and running loss be displayed. 
        dataloader_params : dict, the parameters for the dataloader function. looks like 
            params = {'batch_size': 1,
            'shuffle': True,
            'num_workers': 6}
    
    return : 
        model : Transformer, the transformer trained
        validation_loss_list : list, the list of all validation losses
        running_loss_list : list, the list of all the running losses
    """

    use_cuda = torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")
    torch.backends.cudnn.benchmark = True
    
    torch.manual_seed(seed)
    training_set = Dataset(data_dict)
    training_generator = torch.utils.data.DataLoader(training_set, **dataloader_params)
    del data_dict

    optimizer = optim.Adam(model.parameters(), lr=lr)

    train_loss_list = []
    validation_loss_list = []

    for epoch in range(num_epoch):
        model.train()
        train_cumulative_loss = 0
        for local_batch, local_weights in tqdm(training_generator, desc=f"Epoch {epoch+1}/{num_epoch}", leave=False):
    
            Z, W = local_batch.transpose(0, 1).to(device), local_weights.reshape(-1, 1).to(device)
            W /= torch.sum(W)

            optimizer.zero_grad()
            train_loss = model.loss(Z, W) 
            train_loss.backward()
            optimizer.step()

            train_cumulative_loss += train_loss.item()

        train_cumulative_loss /= len(training_generator)
        model.eval()
        with torch.no_grad():
            ppv_score = -torch.mean(compute_PPV(score(model), struct_file))

        train_loss_list.append(train_cumulative_loss)
        validation_loss_list.append(ppv_score.item())

        if verbose:
            tqdm.write(f"Epoch {epoch+1}: Training Loss = {train_cumulative_loss:.4f}, Validation Loss = {ppv_score.item():.4f}")

    return model, ppv_score.item(), train_loss.item()
